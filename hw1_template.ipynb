{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Te27fi-0pP"
      },
      "source": [
        "# **HW1: Regression**\n",
        "In *assignment 1*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement two regression models to predict the Systolic blood pressure (SBP) of a patient. You will need to implement **both Matrix Inversion and Gradient Descent**.\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implement one regression model to predict the SBP of multiple patients in a different way than the basic part. You can choose **either** of the two methods for this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDdnos-4uUv"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the regression to predict SBP from the given DBP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EVqWlB-DTF"
      },
      "source": [
        "## 1.1 Matrix Inversion Method (25%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_mi.csv**\n",
        "*   Print your coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCR7vk9BFkf"
      },
      "source": [
        "### *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HL5XjqFf4wSj"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnWjrzi0dMPz"
      },
      "source": [
        "### *Global attributes*\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EWLDPOlHBbcK"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_mi.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 3 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['subject_id', 'charttime', 'sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFC-cvqIcYK"
      },
      "source": [
        "You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OUbS2BEgcut6"
      },
      "outputs": [],
      "source": [
        "mape = [] # Mean absolute percentage error for each subject"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoRFoQjBW5S"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dekR1KnqBtI6"
      },
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYPuikLCFx4"
      },
      "source": [
        "### *Implement the Regression Model*\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwdx06JNEYs"
      },
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n",
        "* Validation dataset is used to validate your own model without the testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "USDciENcB-5F"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "\n",
        "    splitidx = int(len(training_datalist)*0.8)\n",
        "    train_dataset = training_datalist[1:splitidx,0]\n",
        "    valid_dataset = training_datalist[1:splitidx,1]\n",
        "    testing1 = training_datalist[splitidx:,0]\n",
        "    answer1 = training_datalist[splitidx:,1]\n",
        "\n",
        "    return train_dataset, valid_dataset, testing1, answer1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3Qln4aNgVy"
      },
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle the unreasonable data\n",
        "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XXvW1n_5NkQ5"
      },
      "outputs": [],
      "source": [
        "def PreprocessData(train_dataset, valid_dataset):\n",
        "    # Remove the Valid_data with value > 250\n",
        "    valid_mean = np.mean(valid_dataset.astype(float))\n",
        "    valid_std = np.std(valid_dataset.astype(float))\n",
        "    zscore = (valid_dataset[:].astype(float) - valid_mean) / valid_std\n",
        "    outlier_mask = np.abs(zscore) > 2\n",
        "\n",
        "    # valid_mask = valid_dataset[:].astype(float) <= 250\n",
        "    train_dataset = train_dataset[~outlier_mask]\n",
        "    valid_dataset = valid_dataset[~outlier_mask]\n",
        "\n",
        "    #Remove the Train dataset with value > 90\n",
        "    train_mask = train_dataset[:].astype(float) <= 120\n",
        "\n",
        "    train_dataset = train_dataset[train_mask]\n",
        "    valid_dataset = valid_dataset[train_mask]\n",
        "\n",
        "    train_mask = train_dataset[:].astype(float) >= 60\n",
        "\n",
        "    train_dataset = train_dataset[train_mask]\n",
        "    valid_dataset = valid_dataset[train_mask]\n",
        "\n",
        "    #Remove the Train dataset with value > 160\n",
        "    valid_mask = valid_dataset[:].astype(float) <= 160\n",
        "    train_dataset = train_dataset[valid_mask]\n",
        "    valid_dataset = valid_dataset[valid_mask]\n",
        "\n",
        "\n",
        "\n",
        "    return train_dataset, valid_dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLpJmQUN3V6"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Matrix Inversion to finish this part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Tx9n1_23N8C0"
      },
      "outputs": [],
      "source": [
        "def MatrixInversion(train_dataset, valid_dataset):\n",
        "\n",
        "    x = train_dataset\n",
        "    x = x.astype(float) # convert data type to float\n",
        "    x = x.reshape((len(x), 1))\n",
        "    ones = np.ones((len(x), 1)) # create an array of 1s with the same number of rows as x\n",
        "    x = np.hstack((ones, x))\n",
        "\n",
        "\n",
        "    y = valid_dataset\n",
        "    y = y.astype(float) # convert data type to float\n",
        "    y = y.reshape((len(y), 1))\n",
        "\n",
        "    x_trans = np.transpose(x)\n",
        "    mult_x = np.dot(x_trans, x)\n",
        "    inv_mult_x = np.linalg.inv(mult_x)\n",
        "    final = np.dot(np.dot(inv_mult_x, x_trans), y) #coeefficents\n",
        "\n",
        "\n",
        "    return final,x,y\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxRNFwyN8xd"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*\n",
        "The final *output_datalist* should look something like this\n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EKlDIC2-N_lk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def MakePrediction(data, coeffs):\n",
        "    prediction = np.dot(data, coeffs)\n",
        "    return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCd0Z6izOCwq"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCL92EPKOFIn",
        "outputId": "28aa941f-eb77-4717-8be6-d2b43a483b80"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "MakePrediction() missing 1 required positional argument: 'c'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Codingan\\2023-2\\Machine Learning\\Assign 1\\hw1_template.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Codingan/2023-2/Machine%20Learning/Assign%201/hw1_template.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m ones \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((test_col\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m)) \u001b[39m# create an array of 1s with the same number of rows as x\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Codingan/2023-2/Machine%20Learning/Assign%201/hw1_template.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m test_col \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack((ones,test_col))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Codingan/2023-2/Machine%20Learning/Assign%201/hw1_template.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m output_datalist \u001b[39m=\u001b[39m MakePrediction(test_col, coeffs) \u001b[39m#Final Result\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Codingan/2023-2/Machine%20Learning/Assign%201/hw1_template.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m######Mape Calculation##########################################################################################\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Codingan/2023-2/Machine%20Learning/Assign%201/hw1_template.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# cekTrain , cekValid = SplitData()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Codingan/2023-2/Machine%20Learning/Assign%201/hw1_template.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m cekTrain \u001b[39m=\u001b[39m soal\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: MakePrediction() missing 1 required positional argument: 'c'"
          ]
        }
      ],
      "source": [
        "train, valid, soal, answer = SplitData()\n",
        "train, valid = PreprocessData(train, valid)\n",
        "coeffs, X, Y = MatrixInversion(train, valid)\n",
        "# newpred=MakePrediction(X, coeffs) #Test Mape\n",
        "\n",
        "#Testing Data (Output Datalist)\n",
        "test_col = testing_datalist[:, 0]\n",
        "test_col = test_col[1:].astype(float)\n",
        "test_col = test_col.reshape((len(test_col), 1))\n",
        "ones = np.ones((test_col.shape[0], 1)) # create an array of 1s with the same number of rows as x\n",
        "test_col = np.hstack((ones,test_col))\n",
        "output_datalist = MakePrediction(test_col, coeffs) #Final Result\n",
        "\n",
        "######Mape Calculation##########################################################################################\n",
        "# cekTrain , cekValid = SplitData()\n",
        "cekTrain = soal.astype(float)\n",
        "cekValid = answer.astype(float)\n",
        "cekTrain = cekTrain.reshape((len(cekTrain), 1))\n",
        "ones = np.ones((cekTrain.shape[0], 1)) # create an array of 1s with the same number of rows as x\n",
        "cekTrain = np.hstack((ones,cekTrain))\n",
        "newpred=MakePrediction(cekTrain, coeffs)\n",
        "mape=[]\n",
        "for i in range(len(newpred)):\n",
        "    mape.append(abs((cekValid[i]-newpred[i]))/abs(cekValid[i]))\n",
        "\n",
        "hasil = np.mean(mape) * 100\n",
        "\n",
        "print(\"MAPE :\"+ str((hasil))+\"%\")\n",
        "print(\"Coefficients: \"+ str(round(coeffs[1][0]))+\" \"+str(round(coeffs[0][0])))\n",
        "##############################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jhd8wAOk3D"
      },
      "source": [
        "### *Write the Output File*\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tYQVYLlKOtDB"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3WOhglA9ML"
      },
      "source": [
        "## 1.2 Gradient Descent Method (30%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_gd.csv**\n",
        "*   Output your coefficient update in a csv file **hw1_basic_coefficient.csv**\n",
        "*   Print your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMqa_xjXhEv"
      },
      "source": [
        "### *Global attributes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wNZtRWUeXpEu"
      },
      "outputs": [],
      "source": [
        "output_dataroot = 'hw1_basic_gd.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "coefficient_output_dataroot = 'hw1_basic_coefficient.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 3 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['subject_id', 'charttime', 'sbp']\n",
        "\n",
        "coefficient_output = [] # Your coefficient update during gradient descent\n",
        "                   # Should be a (number of iterations * number_of coefficient) matrix\n",
        "                   # The format of each row should be ['w0', 'w1', ...., 'wn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5DeHxdLdai3"
      },
      "source": [
        "Your own global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_2IO5tYSdaFd"
      },
      "outputs": [],
      "source": [
        "mapes=[]\n",
        "coefficient_output = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBLT1aqXuW0"
      },
      "source": [
        "### *Implement the Regression Model*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPWpcOnXhCZ"
      },
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1PEf_qGvYHu0"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "\n",
        "\n",
        "    splitidx = int(len(training_datalist)*0.8)\n",
        "    train_dataset = training_datalist[1:splitidx,0]\n",
        "    valid_dataset = training_datalist[1:splitidx,1]\n",
        "    testing1 = training_datalist[splitidx:,0]\n",
        "    answer1 = training_datalist[splitidx:,1]\n",
        "\n",
        "    return train_dataset, valid_dataset, testing1, answer1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSoPDPKX56w"
      },
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uLTXOWRwYHiS"
      },
      "outputs": [],
      "source": [
        "def PreprocessData(train_dataset, valid_dataset):\n",
        "    # Remove the Valid_data with value > 250\n",
        "    valid_mean = np.mean(valid_dataset.astype(float))\n",
        "    valid_std = np.std(valid_dataset.astype(float))\n",
        "    zscore = (valid_dataset[:].astype(float) - valid_mean) / valid_std\n",
        "    outlier_mask = np.abs(zscore) > 2\n",
        "\n",
        "    # valid_mask = valid_dataset[:].astype(float) <= 250\n",
        "    train_dataset = train_dataset[~outlier_mask]\n",
        "    valid_dataset = valid_dataset[~outlier_mask]\n",
        "\n",
        "    #Remove the Train dataset with value > 90\n",
        "    train_mask = train_dataset[:].astype(float) <= 90\n",
        "\n",
        "    train_dataset = train_dataset[train_mask]\n",
        "    valid_dataset = valid_dataset[train_mask]\n",
        "\n",
        "    valid_mask = valid_dataset[:].astype(float) <= 140\n",
        "    train_dataset = train_dataset[valid_mask]\n",
        "    valid_dataset = valid_dataset[valid_mask]\n",
        "\n",
        "\n",
        "\n",
        "    return train_dataset, valid_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_y82gXX6a-"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Gradient Descent to finish this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-635Ee00YHTE"
      },
      "outputs": [],
      "source": [
        "\n",
        "def GradientDescent(X, y, alpha, iterations, n , m , c):\n",
        "    for i in range(iterations):\n",
        "        newpred = m*X + c\n",
        "        d_m = (-2/n) * sum(X * (y - newpred))\n",
        "        d__c = (-2/n) * sum(y - newpred)\n",
        "        m = m - alpha * d_m\n",
        "        c = c - alpha * d__c\n",
        "        coefficient_output.append([m,c])\n",
        "\n",
        "    return m,c\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuPxs2ZX21S"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "\n",
        "Make prediction of testing dataset and store the values in *output_datalist*\n",
        "The final *output_datalist* should look something like this\n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP\n",
        "\n",
        "Remember to also store your coefficient update in *coefficient_output*\n",
        "The final *coefficient_output* should look something like this\n",
        "> [ [1, 0, 3, 5], ... , [0.1, 0.3, 0.2, 0.5] ] where each row contains the [w0, w1, ..., wn] of your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8pnNDlQeYGtE"
      },
      "outputs": [],
      "source": [
        "def MakePrediction(m, X, c):\n",
        "    prediction = m*X+c\n",
        "    return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScbxxMAYAgZ"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90EisOc7YG-N",
        "outputId": "d66c776c-7f89-4d1a-8195-037ee7471e61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAPE :5.447731385014828%\n",
            "Coefficients: 1 34\n"
          ]
        }
      ],
      "source": [
        "coefficient_output = []\n",
        "train, valid, cekTrain, cekValid = SplitData()\n",
        "X, Y = PreprocessData(train, valid)\n",
        "X = X.astype(float) # convert data type to float\n",
        "Y= Y.astype(float) # convert data type to float\n",
        "length = float(len(X))\n",
        "coeff,bias = GradientDescent(X, Y, 0.0001, 200000, length, 0, 0)\n",
        "\n",
        "cekTrain = cekTrain.astype(float) # convert data type to float\n",
        "cekValid = cekValid.astype(float) # convert data type to float\n",
        "\n",
        "newpred = MakePrediction(coeff, cekTrain, bias) #Test Mape\n",
        "\n",
        "\n",
        "test_col = testing_datalist[:, 0]\n",
        "test_col = test_col[1:].astype(float)\n",
        "\n",
        "output_datalist = MakePrediction(coeff, test_col, bias) #Final Result\n",
        "output_datalist = output_datalist.reshape((len(output_datalist),1))\n",
        "\n",
        "######Mape Calculation##########################################################################################\n",
        "mapes=[]\n",
        "for i in range(len(newpred)):\n",
        "    mapes.append(abs((cekValid[i]-newpred[i]))/abs(cekValid[i]))\n",
        "\n",
        "hasil = np.mean(mapes) * 100\n",
        "\n",
        "print(\"MAPE :\"+ str((hasil))+\"%\")\n",
        "print(\"Coefficients: \"+ str(round(coeff))+\" \"+str(round(bias)))\n",
        "##############################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1DpV_HcYFpl"
      },
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "**Write the coefficient update to csv**\n",
        "> Format: 'w0', 'w1', ..., 'wn'\n",
        ">*   The number of columns is based on your number of coefficient\n",
        ">*   The number of row is based on your number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NLSHgpDvDXNI"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)\n",
        "\n",
        "with open(coefficient_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in coefficient_output:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4408qg4xMQ"
      },
      "source": [
        "# **2. Advanced Part (40%)**\n",
        "In the second part, you need to implement the regression in a different way than the basic part to help your predictions of multiple patients SBP.\n",
        "\n",
        "You can choose **either** Matrix Inversion or Gradient Descent method.\n",
        "\n",
        "The training data will be in **hw1_advanced_training.csv** and the testing data will be in **hw1_advanced_testing.csv**.\n",
        "\n",
        "Output your prediction in **hw1_advanced.csv**\n",
        "\n",
        "Notice:\n",
        "> You cannot import any other package other than those given\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gymnKwyaxbg2"
      },
      "source": [
        "### Input the training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "v66HUClZcxaE"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZQxcolWxbg2"
      },
      "source": [
        "### Your Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ldZrrMahd42a"
      },
      "outputs": [],
      "source": [
        "mape =[] # Mean absolute percentage error for each subject\n",
        "tabungan =[]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "666DaeC2d42b"
      },
      "source": [
        "Load Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WYSBxYAixbg3"
      },
      "outputs": [],
      "source": [
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = pd.read_csv(csvfile)\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = pd.read_csv(csvfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode():\n",
        "\n",
        "    for i,encodedMap in enumerate(training_datalist['subject_id'].unique()):\n",
        "        training_datalist['subject_id'].replace(encodedMap,i+1,inplace=True)\n",
        "        testing_datalist['subject_id'].replace(encodedMap,i+1,inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def iqr(datasetku): #IQR\n",
        "    q1 = np.percentile(datasetku, 25)\n",
        "    q3 = np.percentile(datasetku, 75)\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 -(1.5 * iqr)\n",
        "    upper = q3 +(1.5 * iqr)\n",
        "    return upper, lower"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GuKsT9uPd42b"
      },
      "outputs": [],
      "source": [
        "def SplitPreProData():\n",
        "    encode() #Encode the subject_id\n",
        "    dataset = training_datalist[['subject_id','temperature', 'heartrate', 'resprate', 'o2sat', 'sbp']]\n",
        "    test_dataset = testing_datalist[['subject_id','temperature', 'heartrate', 'resprate', 'o2sat']]\n",
        "    dataset = dataset.dropna(subset=['temperature', 'heartrate', 'resprate', 'o2sat'])\n",
        "\n",
        "    upper_resp, lower_resp = iqr(dataset['resprate'])\n",
        "    upper_hr, lower_hr = iqr(dataset['heartrate'])\n",
        "    upper_o2, lower_o2 = iqr(dataset['o2sat'])\n",
        "    upper_temp, lower_temp = iqr(dataset['temperature'])\n",
        "    dataset = dataset[(dataset['resprate'] <= upper_resp) & (dataset['resprate'] >= lower_resp)]\n",
        "    dataset = dataset[(dataset['heartrate'] <= upper_hr) & (dataset['heartrate'] >= lower_hr)]\n",
        "    dataset = dataset[(dataset['o2sat'] >= lower_o2) & (dataset['o2sat'] <= upper_o2)]\n",
        "    dataset = dataset[(dataset['temperature'] >= lower_temp ) & (dataset['temperature'] <= upper_temp)]\n",
        "    train_dataset = dataset[['subject_id','temperature', 'heartrate', 'resprate', 'o2sat']]\n",
        "    valid_dataset = dataset[['sbp']]\n",
        "\n",
        "    train_dataset.insert(0, 'bias', 1)\n",
        "    test_dataset.insert(0, 'bias', 1)\n",
        "\n",
        "\n",
        "    train_data ={}\n",
        "    valid_data = {}\n",
        "    test_data = {}\n",
        "    cekTrain ={}\n",
        "    cekValid = {}\n",
        "   \n",
        "\n",
        "    for subject_id in training_datalist['subject_id'].unique():\n",
        "        \n",
        "        subject_data = train_dataset[train_dataset['subject_id'] == subject_id].drop(columns=['subject_id'])\n",
        "        subject_valid = valid_dataset[train_dataset['subject_id'] == subject_id]\n",
        "        splitidx = int(len(subject_data)*0.8)\n",
        "\n",
        "        train_data[subject_id] = subject_data[:splitidx].values\n",
        "        valid_data[subject_id] = subject_valid[:splitidx].values\n",
        "        cekTrain[subject_id] = subject_data[splitidx:].values #Dataset for Testing MAPE 20%\n",
        "        cekValid[subject_id] = subject_valid[splitidx:].values #Dataset for Testing MAPE 20%\n",
        "\n",
        "    for subject_id in testing_datalist['subject_id'].unique():\n",
        "        test_sub = test_dataset[test_dataset['subject_id'] == subject_id].drop(columns=['subject_id'])\n",
        "        test_data[subject_id] = test_sub.values\n",
        "\n",
        "    \n",
        "        \n",
        "   \n",
        "\n",
        "\n",
        "    return train_data, valid_data, test_data, cekTrain, cekValid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Ww8HgQWud42b"
      },
      "outputs": [],
      "source": [
        "def MatrixInversion(train_dataset, valid_dataset):\n",
        "\n",
        "    x = train_dataset\n",
        "    y = valid_dataset\n",
        "\n",
        "    x_trans = np.transpose(x)\n",
        "    mult_x = np.dot(x_trans, x)\n",
        "    inv_mult_x = np.linalg.inv(mult_x)\n",
        "    final = np.dot(np.dot(inv_mult_x, x_trans), y) #coeefficents\n",
        "\n",
        "    return final,x,y\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "tpQ9-3-Bd42c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def MakePrediction(data, coeffs):\n",
        "    prediction = np.dot(data, coeffs)\n",
        "    return prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYk2wBded42c",
        "outputId": "df53c0d1-930a-4d55-9397-5d91c59feca5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mape  1 : 10.941775306321471 %\n",
            "Mape  2 : 9.480207500298791 %\n",
            "Mape  3 : 15.847571816471028 %\n",
            "Mape  4 : 11.615722886697426 %\n",
            "Mape  5 : 7.957093790008299 %\n",
            "Mape  6 : 15.231450541720209 %\n",
            "Mape  7 : 8.720614405590746 %\n",
            "Mape  8 : 13.839379865620751 %\n",
            "Mape  9 : 13.767378930905746 %\n",
            "Mape  10 : 10.316288730324652 %\n",
            "Mape  11 : 11.286465485305417 %\n",
            "Rata-rata MAPE : 11.727631750842233 %\n"
          ]
        }
      ],
      "source": [
        "train, valid, test, cekTrain, cekValid = SplitPreProData() #Split data dan Hasil Preprocess data yg dipotong outlier\n",
        "model_coeffs = {}\n",
        "Y_valid = {}\n",
        "new_result = {}\n",
        "tabungan = []\n",
        "output_datalist = []\n",
        "\n",
        "for subject_id in training_datalist['subject_id'].unique():\n",
        "    train_ku = train[subject_id].astype(float)\n",
        "    valid_ku = valid[subject_id].astype(float)\n",
        "    model_coeffs[subject_id],X_train,Y_valid[subject_id] = MatrixInversion(train_ku, valid_ku)\n",
        "\n",
        "#Testing Datalist (Output Datalist)\n",
        "for subject_id in testing_datalist['subject_id'].unique():\n",
        "    tabungan.append(MakePrediction(test[subject_id], model_coeffs[subject_id])) #Final Result\n",
        "\n",
        "for i in range(len(tabungan)):\n",
        "    for j in range(len(tabungan[i])):\n",
        "        output_datalist.append(tabungan[i][j])\n",
        "############################################################################################################\n",
        "\n",
        "\n",
        "# Test 20% dataset （MAPE）\n",
        "for subject_id in training_datalist['subject_id'].unique():\n",
        "    new_result[subject_id]=MakePrediction(cekTrain[subject_id], model_coeffs[subject_id]) #Test Mape\n",
        "\n",
        "rata2 = 0\n",
        "for subject_id in training_datalist['subject_id'].unique():\n",
        "    mape = []\n",
        "    for i in range(len(new_result[subject_id])):\n",
        "        mape.append(abs((cekValid[subject_id][i]-new_result[subject_id][i]))/abs(cekValid[subject_id][i]))\n",
        "\n",
        "    hasil = np.mean(mape) * 100\n",
        "    rata2 += hasil\n",
        "    print(\"Mape \", subject_id, \":\", hasil, \"%\")\n",
        "\n",
        "rata2 = rata2/len(training_datalist['subject_id'].unique())\n",
        "print(\"Rata-rata MAPE :\", rata2, \"%\")\n",
        "############################################################################################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Gradient_Descent(x , y , iterations , Alpha, m) :\n",
        "    theta = np.zeros((x.shape[1],1))\n",
        "    for i in range(iterations) :\n",
        "        Prediction = np.dot(x , theta)\n",
        "        Error = Prediction - y\n",
        "        gradient = (1/m) * np.dot(x.T , Error)\n",
        "        theta = theta - Alpha * gradient\n",
        "        \n",
        "    return theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def SplitPreProData():\n",
        "    encode() #Encode the subject_id\n",
        "    dataset = training_datalist[['subject_id','temperature', 'heartrate', 'resprate', 'o2sat', 'sbp']]\n",
        "    test_dataset = testing_datalist[['subject_id','temperature', 'heartrate', 'resprate', 'o2sat']]\n",
        "    dataset = dataset.dropna(subset=['temperature', 'heartrate', 'resprate', 'o2sat'])\n",
        "   \n",
        "    upper_resp, lower_resp = iqr(dataset['resprate'])\n",
        "    upper_hr, lower_hr = iqr(dataset['heartrate'])\n",
        "    upper_o2, lower_o2 = iqr(dataset['o2sat'])\n",
        "    upper_temp, lower_temp = iqr(dataset['temperature']) \n",
        "    dataset = dataset[(dataset['resprate'] <= upper_resp) & (dataset['resprate'] >= lower_resp)]\n",
        "    dataset = dataset[(dataset['heartrate'] <= upper_hr) & (dataset['heartrate'] >= lower_hr)]\n",
        "    dataset = dataset[(dataset['o2sat'] >= lower_o2) & (dataset['o2sat'] <= upper_o2)]\n",
        "    dataset = dataset[(dataset['temperature'] >= lower_temp ) & (dataset['temperature'] <= upper_temp)]\n",
        "    train_dataset = dataset[['subject_id','temperature', 'heartrate', 'resprate', 'o2sat']]\n",
        "    valid_dataset = dataset[['sbp']]\n",
        "\n",
        "    train_dataset.insert(0, 'bias', 1)\n",
        "    test_dataset.insert(0, 'bias', 1)\n",
        "\n",
        "\n",
        "    train_data ={}\n",
        "    valid_data = {}\n",
        "    test_data = {}\n",
        "    cekTrain ={}\n",
        "    cekValid = {}\n",
        "\n",
        "    for subject_id in training_datalist['subject_id'].unique():\n",
        "        subject_data = train_dataset[train_dataset['subject_id'] == subject_id].drop(columns=['subject_id'])\n",
        "        subject_valid = valid_dataset[train_dataset['subject_id'] == subject_id]\n",
        "        splitidx = int(len(subject_data)*0.8)\n",
        "        train_data[subject_id] = subject_data[:splitidx].values\n",
        "        valid_data[subject_id] = subject_valid[:splitidx].values\n",
        "        cekTrain[subject_id] = subject_data[splitidx:].values\n",
        "        cekValid[subject_id] = subject_valid[splitidx:].values\n",
        "\n",
        "    for subject_id in testing_datalist['subject_id'].unique():\n",
        "        test_sub = test_dataset[test_dataset['subject_id'] == subject_id].drop(columns=['subject_id'])\n",
        "        test_data[subject_id] = test_sub.values\n",
        "        \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    return train_data, valid_data, test_data, cekTrain, cekValid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def MakePrediction(data, coeffs):\n",
        "    prediction = np.dot(data, coeffs)\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "13.32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mape  1 : 11.02448313717266 %\n",
            "Mape  2 : 9.664178182621333 %\n",
            "Mape  3 : 16.141628275984218 %\n",
            "Mape  4 : 11.751044630760315 %\n",
            "Mape  5 : 7.795393528074119 %\n",
            "Mape  6 : 14.979934518836993 %\n",
            "Mape  7 : 8.533093063216443 %\n",
            "Mape  8 : 13.200718902650504 %\n",
            "Mape  9 : 13.744032411460392 %\n",
            "Mape  10 : 10.320047059151815 %\n",
            "Mape  11 : 11.41715892428236 %\n",
            "Rata-rata MAPE : 11.688337512201015 %\n"
          ]
        }
      ],
      "source": [
        "train, valid, test, cekTrain, cekValid = SplitPreProData() #Split data dan Hasil Preprocess data yg dipotong \n",
        "alpha = 0.000001\n",
        "myTheta ={}\n",
        "new_result = {}\n",
        "for subject_id in training_datalist['subject_id'].unique():\n",
        "    m = len(train[subject_id])\n",
        "    myTheta[subject_id] = Gradient_Descent(train[subject_id], valid[subject_id],500000, alpha, m)\n",
        "\n",
        "#Testing Datalist (Output Datalist)\n",
        "for subject_id in testing_datalist['subject_id'].unique():\n",
        "    tabungan.append(MakePrediction(test[subject_id], myTheta[subject_id])) #Final Result\n",
        "\n",
        "for i in range(len(tabungan)):\n",
        "    for j in range(len(tabungan[i])):\n",
        "        output_datalist.append(tabungan[i][j])\n",
        "\n",
        "\n",
        "#Make Prediction (MAPE Calculation)\n",
        "############################################################################################################\n",
        "\n",
        "for subject_id in training_datalist['subject_id'].unique():\n",
        "    new_result[subject_id]=MakePrediction(cekTrain[subject_id], myTheta[subject_id]) #Test Mape\n",
        "\n",
        "rata2 = 0\n",
        "for subject_id in training_datalist['subject_id'].unique():\n",
        "    mape = []\n",
        "    hasil = 0\n",
        "    for i in range(len(new_result[subject_id])):\n",
        "        mape.append(abs((cekValid[subject_id][i]-new_result[subject_id][i]))/abs(cekValid[subject_id][i]))\n",
        "\n",
        "    hasil = np.mean(mape) * 100\n",
        "    rata2 += hasil\n",
        "    print(\"Mape \", subject_id, \":\", hasil, \"%\")\n",
        "\n",
        "rata2 = rata2/len(training_datalist['subject_id'].unique())\n",
        "print(\"Rata-rata MAPE :\", rata2, \"%\")\n",
        "############################################################################################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aWssBKmxbg3"
      },
      "source": [
        "### Output your Prediction\n",
        "\n",
        "> your filename should be **hw1_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "wQkfO_q7xbg3"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgCJU7FPeJL"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw1_report.pdf**\n",
        "\n",
        "*   Briefly describe the difficulty you encountered\n",
        "*   Summarize your work and your reflections\n",
        "*   No more than one page\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEE53_MPf4W"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_wDdnos-4uUv"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
